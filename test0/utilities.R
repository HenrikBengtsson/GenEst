require(R6)logit <- function(x) log(x/(1-x))alogit <- function(x) 1/(1+exp(-x))
# This function creates a data set with columns given in vars. There is a row for# every unique combination of the levels of vars found in dat. A final column# gives a unique code which is the concatenation of the labels in the other columns.# The rows define the unique cells given by combinations of categories in vars.# If you want only one group, do: make_egDat(NULL,dat)make_egDat <- function(vars, dat) {  nvars <- length(vars) # Should be 0, 1, or 2 for now, but function will work for more.  if (nvars == 0) {    return(data.frame(group="all",cellNames="all"))  } else {      if(any(is.na(match(vars,names(dat))))) {        stop("At least one covariate in the user-specified model is not found in data.")      }      varNames <- sort(vars)      varLabels <- list()      varNlevels <- list()      for (i in 1:nvars) {        varLabels[[i]] <- levels(as.factor(dat[[varNames[i]]]))        varNlevels[[i]] <- length(varLabels[[i]])      }      reps <- cumprod(varNlevels)[nvars:1] # Reverse cumulative product      egDat <- data.frame(var1=gl(varNlevels[[1]],1,length=reps[1],labels=varLabels[[1]]))      if (nvars > 1) {        for (j in 2:nvars) {          egDat[[paste("var",j,collapse=NULL)]] <- gl(varNlevels[[j]],reps[j],length=reps[1],labels=varLabels[[j]])        }      }    }  names(egDat) <- varNames  egDat$cellNames <- apply(egDat, 1, paste0, collapse="")  return(egDat)}
############# pkMod class for data management## The data (including model form) are read into the class and preprocessed upon instantiation.## Fitting and simulation are then done in separate calls.## Instantiation requires:##  dataframe with s1 column required; s2, s3, ... and covariates optional; other cols OK   # search columns s1, s2, ... and covariates. Search result cols are assumed to be in   # chronological order, with leftmost column earliest## pvars and kvars are either character vectors with the names of the covariates, in which case## all the covariates must be included among the covariate columns of dat,## or 1, indicating that there are no covariates.##  pop, kop = '*' or '+' if 2 vars are given; ignored otherwise##  k = user-specified k value (required if kvars is not given; ignored otherwise)pkMod <- R6Class("pkMod",  portable = FALSE, # can't access from other packages, but streamlines coding  public = list(    dat = NULL, pvars = NULL, kvars = NULL, pop = NULL, kop = NULL, k = NULL,    fp = NULL, fk = NULL, scols = NULL, zeros = NULL, found = NULL, maxmiss = NULL,    miniXp = NULL, miniXk = NULL, np = NULL, nk = NULL, facts = NULL,  groups = NULL,    theta = NULL, vartheta = NULL, egDat = NULL, Xp = NULL, Xk = NULL, aic = NULL,    convergence = NULL, pname = NULL, kname = NULL,    initialize = function(dat, pvars = 1, pop = NULL, kvars = NULL, kop = NULL, k = NULL,      fit = T, getVar = T){      dat <<- dat      # <<- assignment statement puts values into class environment rather than workspace      # If portable = TRUE, assignments must be via, e.g., self$dat <- dat and references to      # data within the class must also be done via self$. The portable = FALSE option allows      # data to be accessed directly and assigned with <<- rather than self$      pvars <<- sort(pvars); self$pop <- ifelse(length(pvars) > 1, pop, ' ')      self$kop <- ifelse(length(kvars) > 1, kop, ' ')      # NOTE: minimal error-checking has been implemented on the reading and parsing of inputs      #   (assumption is that the data are correct)      k <<- k;      if (!is.null(kvars) && !is.na(kvars)) {        kvars <<- sort(kvars)        fk <<- as.formula(paste("~", paste(kvars, collapse = self$kop)))        kname <<- paste('k', paste(as.character(fk), collapse = ' '))      } else {        kop <<- ''        kname <<- paste0('k = ', k)      }      if (is.numeric(pvars)){        fp <<- ~ 1      } else {        fp <<- as.formula(paste("~", paste(sort(pvars), collapse = self$pop)))      }      pname <<- paste('p', paste(as.character(fp), collapse = ' '))      # Identify columns containing search outcome data      scols <<- grep("s\\d{1}",names(dat), ignore.case=TRUE)      Xp <<- model.matrix(fp, dat)      np <<- dim(Xp)[2]      allcov <- sort(unique(c(pvars[is.character(pvars)], kvars[is.character(kvars)])))      egDat <<- make_egDat(allcov, dat)      miniXp <<- model.matrix(fp, egDat)      if (!is.null(kvars) && !sum(suppressWarnings(is.na(kvars))) > 0) {        Xk <<- model.matrix(fk, dat)        # Create the zeros vector, which counts the number of times the given carcass        # was missed in the searcher efficiency field trials.        zeros <<- matrixStats::rowCounts(as.matrix(dat[, scols]), value = 0, na.rm = T)
        # Create the found vector. It has length equal to NROWS(dat). Each element        # gives the search occasion when the carcass is found and is 0 if the        # carcass is never found.        foundInd <- which(matrixStats::rowCounts(as.matrix(dat[, scols]), value = 1, na.rm = T) == 1)        found <<- numeric(length(zeros))        found[foundInd] <<- zeros[foundInd] + 1        miniXk <<- model.matrix(fk, egDat)        nk <<- dim(Xk)[2]        maxmiss <<- max(zeros)        facts <<- cbind(miniXp, miniXk)        nfact <- dim(facts)[1]        tXpk <- t(cbind(Xp, Xk))        groups <<- numeric(dim(Xp)[1])        for (i in 1:nfact) groups[colSums(tXpk == facts[i,]) == np + nk] <<- i      } else {        nk <<- 0        groups <<- numeric(dim(Xp)[1])        tXp <- t(Xp)        for (i in 1:np) groups[colSums(tXp == miniXp[i,]) == np] <<- i      }      if (fit) pkreg(getVar = getVar)    },    pkreg = function(theta = NULL, getVar = FALSE){      # fit the user-specified pk model for the given data set      # This function is not performed upon instantiation. Rationale is to give user two      # additional options:      #   1) to provide a custom starting point for optimization (theta), which can be used for      #    potentially faster fitting or for research purposes, and      #   2) extracting the variance-covariance matrix of parameter estimates, which is      #    necessary for simulation; however, it is costly in terms of computation time,      #    but not necessary for model selection (so, set getVar = FALSE for model selection)      if (is.null(kvars) || sum(suppressWarnings(is.na(kvars))) > 0){        # k is provided as a specified constant: k=0 => Huso; k=1 => Shoenfeld (if CPdist = exp)        # use only the s0 column to estimate p via logistic regression        result <<- glm(          as.formula(paste('s1 ~', paste(sort(pvars), collapse = pop))),          data = dat[dat$s1 %in% 0:1,],          family = 'binomial'        )        theta <<- result$coef        vartheta <<- summary(result)$cov.unscaled        return(list(          pmodel = paste(fp, collapse = ''),          kmodel = paste(fk, collapse = ''),          betaphat = result$coef,          betakhat = logit(k),          vartheta = summary(result)$cov.unscaled,          aic = result$aic,          convergence = result$converged          )        )      } else {        if (missing(theta)){          # Then get least squares estimates for the betas. The first NCOL(Xp)          # elements of theta are starting values for the betas in the p model. The          # remaining NCOL(Xk) elements are the betas for the k model assuming a          # constant k = 0.7          ngroups <- NCOL(Xp)          empp <- numeric(nrow(Xp))          if (sum(attr(terms(fk), "term.labels") %in% attr(terms(fp), "term.labels")) <            length(attr(terms(fk), "term.labels"))) {              tXp <- t(Xp)              pgroups <- numeric(dim(Xp)[1])              for (i in 1:dim(Xp)[2]) pgroups[colSums(tXp == miniXp[i,]) == np] <- i          }          for (i in 1:dim(miniXp)[1]) {            empp[which(groups==i)] <- mean(dat$s1[which(groups == i & dat$s1 >= 0)], na.rm=TRUE)          }          empp[which(empp==0)] <- 0.1 # Cells with all 0's set to 0.1          empp[which(empp==1)] <- 0.9 # Cells with all 1's set to 0.1          theta <- c(solve(t(Xp)%*%Xp)%*%t(Xp)%*%logit(empp), logit(rep(0.7,times=NCOL(Xk))))        }        result <- optim(par = theta, fn = function(theta){          Beta <- array(numeric(length(theta) * 2), dim = c(length(theta), 2))          Beta[1:np,1] <- theta[1:np]          Beta[(np+1):length(theta), 2]<-theta[(np+1):length(theta)]          pk <- alogit(facts %*% Beta)          powk<-array(rep(pk[, 2], maxmiss + 1), dim=c(dim(pk)[1], maxmiss+1))          powk[,1] <- 1          powk <- matrixStats::rowCumprods(powk)          pmiss <- matrixStats::rowCumprods(matrix(1 - (pk[,1]*powk[,1:maxmiss]), nrow = dim(pk)[1]))          pfind.si <- cbind(pk[,1], matrixStats::rowDiffs(1-pmiss))          -(sum(log(pmiss[cbind(groups[found == 0], zeros[found == 0])]))+sum(log(pfind.si[cbind(groups[found > 0], found[found > 0])])))        }, method = "BFGS", hessian = getVar)        betaphat <- result$par[1:NCOL(Xp)]        betakhat <- result$par[(NCOL(Xp)+1):length(theta)]        theta <<- c(betaphat, betakhat)        if(getVar) {          vartheta <<- solve(result$hessian)        } else {          vartheta <<- NA        }      }      aic <<- 2*result$value + 2*length(result$par)      convergence <<- result$convergence      return(list(        pmodel = paste(fp, collapse = ''),        kmodel = paste(fk, collapse = ''),        betaphat = betaphat,        betakhat = betakhat,        vartheta = vartheta,        aic = aic,        convergence = result$convergence      ))    },    pksim = function(nsim){      # simulate two columns of p and k parameters for each combination of covariate levels      # E.g., if the covariates levels for cov1 are E and M and for cov2 are s and m, then      # p and k parameters are simulated for cells Es, Em, Ms, and Mm. The values in each may or      # may not be correlated, depending on the model selected.      # The output is formatted for use in routines for fatality estimation routines.      if (is.null(vartheta) || (sum(suppressWarnings(is.na(vartheta))) > 0)) pkreg(getVar = T)      betaSim <- mvtnorm::rmvnorm(nsim , mean = theta, sigma = vartheta)      pSim <- alogit(betaSim[,1:np]%*%t(miniXp))      if (is.null(kvars) || sum(suppressWarnings(is.na(kvars))) > 0){        kSim <- array(k, dim = dim(pSim))      } else {        kSim <- alogit(betaSim[,(np + 1):(np + nk)]%*%t(miniXk))      }      colnames(pSim) <- egDat$cellNames      colnames(kSim) <- egDat$cellNames      return(list(pSim = pSim, kSim = kSim))    },    cellStats = function(vars = NULL){      if (is.null(vartheta) || is.na(vartheta)) pkreg(theta = theta, getVar = T)      if (missing(vars)){        xp <- miniXp        xk <- miniXk        egdat <- egDat      } else {        if (prod(c(pvars, kvars) %in% c(vars, 1)) == 0){          stop("full model missing covariates")        }        egdat <- make_egDat(vars, self$dat)        xp <- model.matrix(fp, egdat)        xk <- model.matrix(fk, egdat)      }      pMu <- xp %*% theta[1:np]      pVar <- xp %*% vartheta[1:np, 1:np] %*% t(xp)      cstats <- list()      cstats[['p']] <- data.frame(array(dim = c(dim(egdat)[1], 5))) # five number summary      names(cstats[['p']]) <- c('q0.025', '1st Qu.', 'median', '3rd Qu.', 'q0.975')      row.names(cstats[['p']]) <- egdat$cellNames      cstats[['k']] <- cstats[['p']]      qs <- c(0.025, 0.25, 0.5, 0.75, 0.975)      xp <- model.matrix(fp, egdat)      pMu <- xp %*% theta[1:np]      pVar <- xp %*% vartheta[1:np, 1:np] %*% t(xp)
      kMu <- xk %*% theta[(np+1):(np+nk)]      kVar <- xk %*% vartheta[(1+np):(np + nk), (1+np):(np + nk)] %*% t(xk)      for (stati in 1:5){        cstats[['p']][, stati] <- alogit(qnorm(qs[stati], mean = pMu, sd = sqrt(diag(pVar))))        cstats[['k']][, stati] <- alogit(qnorm(qs[stati], mean = kMu, sd = sqrt(diag(kVar))))      }      return(cstats)    }  ),
)
# Function to simulate carcass persistence (CP) parameters. Input is# nsims = number of simulations, survOut = a survreg object, and# egDat = output from make_egDat().# Output is a list containing pdb and pda. These are matrices with nsims rows# and number of columns equal to the number of "cells" defined by the (at most)# two categorical variables used in the model.# Each column of pdb contains nsims values simulated from the distribution of# pdb for the corresponding cell. The columns of the pda matrix are identical# since that parameter is the same for all cells. If the survival distribution# is exponential, then the pda's are all 1's.## A couple examples: The CP data set is from CPmod.R.# survOut <- survival::survreg(surv ~ spec*vis, dist = "exp", data = CP)# survOut <- survival::survreg(surv ~ spec*vis, dist = "weibull", data = CP)# survOut <- survival::survreg(surv ~ spec*vis, dist = "lognormal", data = CP)## egDat <- make_egDat(vars=all.vars(formula(survOut))[-1],dat=CP)# sims <- simCP(20,survOut,egDat)# sims$pdbSim# sims$pdaSim
simCP <- function(nsims,survOut,egDat) {  Xcp <- model.matrix(as.formula(paste("~",as.character(formula(survOut))[3],collapse=NULL)),                      data=egDat) # Extract formula and create model matrix.  if (survOut$dist=="exponential") {    # Exponential model's scale is 1.    thetahat <- survOut$coefficients  } else {    # For Weibull, loglogistic, and lognormal, the parameter vector consists of    # the coefficients of the predictor variables followed by log(scale).    thetahat <- c(survOut$coefficients,log(survOut$scale))  }  thetaSim <- mvtnorm::rmvnorm(nsims,mean=thetahat,sigma=survOut$var)  # Models and reparametrizations.  # exponential: a = 1; b = meanCP = exp(mod.e$coef)  # Weibull: a = shape = 1/mod.w$scale; b = scale = exp(mod.w$coef*Xcp)  # log-logistic: a = shape = 1/mod.ll$scale; b = scale = exp(mod.ll$*coef*Xcp)  # lognormal: a = sdlog^2 = mod.ln$scale^2; b = meanlog = mod.lm$coef*Xcp  pdaSim <- switch(survOut$dist,                   exponential=matrix(1,nrow=nsims,ncol=NROW(Xcp)),                   weibull=matrix(1/exp(thetaSim[,NCOL(thetaSim)]),nrow=nsims,ncol=NROW(Xcp)),                   loglogistic=matrix(1/exp(thetaSim[,NCOL(thetaSim)]),nrow=nsims,ncol=NROW(Xcp)),                   lognormal=matrix(exp(thetaSim[,NCOL(thetaSim)])^2,nrow=nsims,ncol=NROW(Xcp)))  pdbSim <- switch(survOut$dist,                   exponential=exp(thetaSim[,1:NCOL(Xcp)]%*%t(Xcp)),                   weibull=exp(thetaSim[,1:NCOL(Xcp)]%*%t(Xcp)),                   loglogistic=exp(thetaSim[,1:NCOL(Xcp)]%*%t(Xcp)),                   lognormal=thetaSim[,1:NCOL(Xcp)]%*%t(Xcp))  colnames(pdbSim) <- egDat$cellNames  colnames(pdaSim) <- egDat$cellNames  return(list(pdaSim=pdaSim,pdbSim=pdbSim))}################################################################################# gvec#          for calculating detection probability in a single class#          given search schedule and simulated columns of carcass persistence#          times and pk parameters## ARGUMENTS#   days = vector of search times, beginning at 0#   CPab = simulated array of persistence parameters#     2-d if Weibull, loglogistic, or lognormal#     vector if exponential#     parameterizations:#       for exponential -- means#       for Weibull, loglogistic -- R's shape and scale (like EoA)#       for lognormal -- shape & scale (like EoA) = sqrt(sdlog) & meanlog in R parameterization#   seef = searcher efficiency parameters#     if k not derived from a pk model of field trial data, then seef is a#       vector of simulated searcher efficiencies and k is a constant defined#       by the user#     if k is derived from field trial data, then seef is an nsim x 2 array of#       simulated p and k## VALUE#   A vector of simulated detection probabilities#################################################################################
  gvec <- function(days, CPab, persdist, seef, k = NULL){#: updated to accommodate four persistence distributions    nsim <- dim(CPab)[1]    samtype <- ifelse(length(unique(diff(days))) == 1, "Formula", "Custom")    nsearch <- length(days) - 1    if(persdist %in% c("Exponential", "exponential")){      pdb <- CPab      pda <- 1/pdb      pdb0 <- exp(mean(log(pdb)))      pda0 <- 1/pdb0    } else {      pda <- CPab[, 1]      pdb <- CPab[, 2]      if (persdist %in% c("Weibull", "weibull", "Log-logistic", "loglogistic")){        pdb0 <- exp(mean(log(pdb)))        pda0 <- 1/mean(1/pda)      } else if (persdist %in% c("Lognormal", "lognormal")){        pdb0 <- mean(pdb)        pda0 <- mean(sqrt(pda))^2      }    }    pk <- array(dim = c(nsim, 2))    if (is.vector(seef)){      pk[, 1] <- seef      pk[, 2] <- k    } else{      pk <- seef    }
    # setting estimation control parameters    #  search limit: number of searches after arrival to include in estimate    #      of seef [when the number of searches is high, including them all in    #      the estimation is calculation intensive but does not contribute    #      signficantly to the result]
      f0 <- mean(pk[, 1])      k0 <- mean(pk[, 2])      ind1 <- rep(1:nsearch, times = nsearch:1)      ind2 <- ind1+1      ind3 <- unlist(lapply(1:nsearch, function(x) x:nsearch)) + 1      schedule.index <- cbind(ind1,ind2,ind3)      schedule <- cbind(days[ind1],days[ind2],days[ind3])      nmiss <- schedule.index[,3] - schedule.index[,2]      maxmiss <- max(nmiss)      powk <- cumprod(c(1, rep(k0, maxmiss))) # vector of k^i's      notfind <- cumprod(1 - f0*powk[-length(powk)])      nvec <- c(1, notfind) * f0
    # conditional probability of finding a carcass on the ith search (row)    # after arrival for given (simulated) searcher efficiency (column):
      pfind.si <- nvec * powk
    # persistences:
      intxsearch <- unique(cbind(schedule[,2] - schedule[,1],                           schedule[,3] - schedule[,2]), MAR = 1)      ppersu <- ppersist(persdist,                         t_arrive0 = 0,                         t_arrive1 = intxsearch[,1],                         t_search = intxsearch[,1] + intxsearch[,2],                         pda = pda0, pdb = pdb0                         )
      arrvec <- (schedule[,2] - schedule[,1])/max(days)      prob_obs <- numeric(dim(schedule)[1])
      for (i in 1:length(prob_obs)){
        prob_obs[i] <- pfind.si[nmiss[i]+1] *                     ppersu[which(abs(intxsearch[,1] -                                    (schedule[i,2] - schedule[i,1])) < 0.001 &                                  abs(intxsearch[,2] - (schedule[i,3] -                                       schedule[i,2])) < 0.001                         ),] * arrvec[i]
      }
      ggnm <- numeric(maxmiss+1)
      for (missi in 0:maxmiss){        ggnm[missi+1] <- sum(prob_obs[nmiss==missi])      }
    # if more than 10 searches, consider truncating search schedule because    #   very few carcasses will be found after being missed 9 or more times,    #   but many searches is costly in terms of calculation efficiency
      if (nsearch > 10){        iskip <- min(which(cumsum(ggnm)/sum(ggnm) > 0.99)) + 1
        # cutting off the search schedule introduces a slight bias.        # Correct by multiplying the final g's by gadj = sum(ggnm)/ggnm[iskip]        gadj <- sum(ggnm)/sum(ggnm[1:iskip])
      } else{        iskip <- maxmiss        gadj <- 1      }
    # estimation of g    # subset the search schedule    # ignoring probabilities of detection carcasses after they have been    #   missed several times):
      schedule <- cbind(days[ind1], days[ind2],                        days[ind3])[ind2 >= ind3 - iskip + 1,]
      # columns for arrival interval and search number:
        schedule.index <- cbind(ind1, ind2, ind3)[ind2 >= ind3 - iskip + 1,]        nmiss <- schedule.index[,3] - schedule.index[,2]        maxmiss <- max(nmiss)
      # searcher efficiencies        if (maxmiss == 0) {          pfind.si <- pk[,1]        } else if (maxmiss == 1){          pfind.si<-cbind(pk[,1], (1 - pk[,1]) * pk[,2]*pk[,1])        } else {          powk <- array(rep(pk[, 2], maxmiss + 1), dim = c(nsim, maxmiss+1))          powk[,1] <- 1          powk <- matrixStats::rowCumprods(powk)          pfind.si <- pk[,1] * powk * cbind(rep(1, nsim),                        matrixStats::rowCumprods(1 - (pk[,1] * powk[, 1:maxmiss])))        }
      intxsearch <- unique(cbind(schedule[,2] - schedule[,1],                     schedule[,3] - schedule[,2]), MAR = 1)      ppersu <- ppersist(persdist,                          t_arrive0 = 0,                          t_arrive1 = intxsearch[,1],                          t_search = intxsearch[,1] + intxsearch[,2],                          pda = CPab[,1], pdb = CPab[,2])
      # assume uniform arrivals        arrvec <- (schedule[,2]-schedule[,1])/max(days)
      # add the probabilities        prob_obs <- numeric(nsim)        if (maxmiss > 0){          for (i in 1:dim(schedule)[1]){            prob_obs <- prob_obs +                          pfind.si[,nmiss[i]+1] *                          ppersu[which(                            abs(intxsearch[,1] - (schedule[i,2] -                                 schedule[i,1])) < 0.001 &                            abs(intxsearch[,2] - (schedule[i,3] -                                 schedule[i,2])) < 0.001),                          ] * arrvec[i]          }        } else {          for (i in 1:dim(schedule)[1]){            prob_obs <- prob_obs +                            pfind.si[nmiss[i]+1] *                            ppersu[which(                              abs(intxsearch[,1] - (schedule[i,2] -                                 schedule[i,1])) < 0.001 &                              abs(intxsearch[,2] - (schedule[i,3] -                                 schedule[i,2])) < 0.001),                            ] * arrvec[i]          }        }    # g for monitored period      gadj <- ifelse(max(prob_obs) <= 1/gadj, gadj, 1)      prob_obs * gadj  }
ppersist <- function(persistence_distn, t_arrive0, t_arrive1, t_search, pdb, pda = NULL, ...){  if(persistence_distn %in% c("Weibull", "weibull")){    return(t((pgamma(outer(1/pdb, t_search - t_arrive0)^pda, 1/pda) -            pgamma(outer(1/pdb, t_search - t_arrive1)^pda, 1/pda)) *            gamma(1 + 1/pda) * outer(pdb, 1/(t_arrive1 - t_arrive0))))  }  if(persistence_distn %in% c("Exponential", "exponential")){    return((exp(outer(t_arrive1 - t_search, 1/pdb)) - exp(outer(t_arrive0 -            t_search, 1/pdb)))/(outer(t_arrive1 - t_arrive0, 1/pdb)))  }  if(persistence_distn %in% c("Lognormal", "lognormal")){    root_pda <- sqrt(pda)    exp_value <- exp((pda/2) + pdb)    tt <- t_search - t_arrive0    part0 <- t(pnorm(outer(pdb, -log(tt), "+") / root_pda)) * tt +             t(pnorm(outer(-pdb, log(tt), "+") / root_pda - root_pda) * exp_value)    tt <- t_search - t_arrive1    part1 <- t(pnorm(outer(pdb, -log(tt), "+")/root_pda)) * tt +             t(pnorm(outer(-pdb, log(tt), "+")/root_pda - root_pda) * exp_value)    return(-(part1 - part0)/(t_arrive1 - t_arrive0))  }  if(persistence_distn %in% c("Log-Logistic", "loglogistic")) {    return(Vectorize(function(t_arrive0, t_arrive1, t_search, pda, pdb){      t1 <- t_search-t_arrive1      t0 <- t_search-t_arrive0      part1 <- ifelse(t1 == 0, 0, t1/(1 + (t1 / pdb)^pda) * hyperg_2F1(1,                1, 1 + 1/pda, 1 / (1 + (t1/pdb)^(-pda))))      part0 <- t0/(1 + (t0/pdb)^pda)*hyperg_2F1(1, 1, 1 + 1/pda,                1/(1 + (t0/pdb)^(-pda)))      -(part1 - part0)/(t_arrive1 - t_arrive0)      },      vectorize.args = c('pdb', 'pda'))(t_arrive0, t_arrive1, t_search, pda, pdb))  }}
